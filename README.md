# Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA

This repo provides the resource of our paper which introduces an LLM-assisted multimodal retrieval method emphasizing question-relevant key visual entities during retrieval, to enhance RA-VQA systems.

The generated key visual entities of OK-VQA dataset can be found in `./data/` directory. We plan to release the experiment of Infoseek dataset, and the full code after completing a follow-up to this paper.

we highlight https://github.com/LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering, the repository on which our work is based.
If you are interested in our work, you can also refer to the code in that repository.



